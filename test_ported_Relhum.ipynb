{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf765e-36ac-4dd6-a450-4bbda8900b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import unittest\n",
    "from test.test_meteorology import Test_relhum\n",
    "\n",
    "import cupy as cp\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.geocat.comp.meteorology as geo\n",
    "import xarray as xr\n",
    "csvpath = \"relhum_ported_test_numpy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb53418-ba38-451d-81d7-8bfa93fa5830",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48da88f-2a8a-4683-8b68-0ae7faaea2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot(allData,name):\n",
    "    arraysizes = np.unique(allData['ArraySize'])\n",
    "    sd_numpy = np.zeros(len(arraysizes))\n",
    "    sd_cupy = np.zeros(len(arraysizes))\n",
    "    y_numpy = np.zeros(len(arraysizes))\n",
    "    y_cupy = np.zeros(len(arraysizes))\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    for i in range(0,len(arraysizes)):\n",
    "        cupydata = allData.loc[(allData['ArraySize'] == arraysizes[i]) & (allData['Approach'] == 'cupy')]\n",
    "        numpydata = allData.loc[(allData['ArraySize'] == arraysizes[i]) & (allData['Approach'] == 'numpy')]\n",
    "        y_cupy[i] = np.mean(cupydata['Runtime(s)'])\n",
    "        y_numpy[i] = np.mean(numpydata['Runtime(s)'])\n",
    "        sd_cupy[i] = np.std(cupydata['Runtime(s)'])\n",
    "        sd_numpy[i] = np.std(numpydata['Runtime(s)'])\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.errorbar(arraysizes, y_numpy, yerr=sd_numpy, fmt='-o',label='numpy',markersize=20)\n",
    "    ax.errorbar(arraysizes, y_cupy, yerr=sd_cupy, fmt='-D',label='cupy',markersize=20)\n",
    "    ax.legend();  # Add a legend.\n",
    "    ax.set_xlabel('ArraySize')  # Add an x-label to the axes.\n",
    "    ax.set_ylabel('Runtime(s)')  # Add a y-label to the axes.\n",
    "    ax.set_title((\"Test_relhum\"))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    plt.savefig(name,dpi=fig.dpi)\n",
    "def test_validation(res_numpy,res_cupy):\n",
    "    assert np.allclose(res_numpy,res_cupy, atol=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9687f-c6cb-4120-8c69-3b546a655240",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9aac68-67b0-4744-ab1a-f8d43c0c4807",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Local CUDA Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973898e-0874-421f-aa6b-4ac75af9811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed as dd\n",
    "#client = dd.Client()\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()\n",
    "client = dd.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cc087-15ae-415a-ac78-e6587cd24b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08f59f-7afa-4b3d-b4a9-1d72e3cb2fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d0ac93b-f893-4071-81cb-7d531098c60c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CPU cluster on PBS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f33d62-c47c-4559-b598-9d0fa60c3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.array as da\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "clusterCPU = PBSCluster(memory='200 GB',\n",
    "                     processes=36,\n",
    "                     cores=36,\n",
    "                     queue='casper',\n",
    "                     walltime='02:00:00',\n",
    "                     resource_spec='select=1:ncpus=36:mem=200gb')\n",
    "print(clusterCPU.job_script())\n",
    "clusterCPU.scale(1)\n",
    "client = Client(clusterCPU)\n",
    "#cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8b0b8-eb23-44d0-8a31-c668b7cf097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61315414-5253-42a5-9ca4-56e432ac6052",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDA cluster on PBS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4e500-7e52-40c9-9875-ed064d303005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.array as da\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "clusterCUDA = PBSCluster(memory='200 GB',\n",
    "                     processes=1,\n",
    "                     cores=1,\n",
    "                     queue='casper',\n",
    "                     walltime='02:00:00',\n",
    "                     resource_spec='select=1:ncpus=1:ngpus=1:mem=200gb')\n",
    "print(clusterCUDA.job_script())\n",
    "clusterCUDA.scale(1)\n",
    "client = Client(clusterCUDA)\n",
    "#cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bc70b-01e4-4802-8a06-6e7774ce962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCPU.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b086ad-4d93-47ac-8f68-633c599522b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a6dce-2862-42fd-b577-fd0d3b57a430",
   "metadata": {},
   "source": [
    "## Initializing Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3103bd5-f76e-4fd8-9531-f1bbccb99aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power = 8\n",
    "chunksize = 10**5\n",
    "p_def_arrays = []\n",
    "t_def_arrays = []\n",
    "q_def_arrays = []\n",
    "for i in range(8,9):\n",
    "    ArraySize = 10**i\n",
    "    p_def_arrays.append(xr.DataArray(np.random.uniform(low=2000,high=100800,size=ArraySize)))\n",
    "    t_def_arrays.append(xr.DataArray(np.random.uniform(low=194.65,high=302.45,size=ArraySize)))\n",
    "    q_def_arrays.append(xr.DataArray(np.random.uniform(low=0,high=0.02038,size=ArraySize)))\n",
    "numpy_results = []\n",
    "cupy_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f57ff7-fb58-43f3-be12-61af00641488",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Numpy input and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d383dec-7760-4e14-bdc5-3fd881b00809",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np\n",
    "for i in range(8,9):\n",
    "    p_def = p_def_arrays[i-1].data\n",
    "    q_def = q_def_arrays[i-1].data\n",
    "    t_def = t_def_arrays[i-1].data\n",
    "    rh_gt_2 = geo.relhum(t_def, q_def, p_def, use_gpu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fbf02-f23f-44f1-a917-b276b32036da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Numpy input and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604910b-524e-4f82-94ca-216c231f01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    p_def = p_def_arrays[i-1].data\n",
    "    q_def = q_def_arrays[i-1].data\n",
    "    t_def = t_def_arrays[i-1].data\n",
    "    rh_gt_2 = geo.relhum(t_def, q_def, p_def, use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4fd4c-2b40-47aa-a090-6ce513e4b420",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Xarray input and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42547a9d-22f4-41b3-8dd5-6d5b0d360e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    p_def = p_def_arrays[i-1]\n",
    "    q_def = q_def_arrays[i-1]\n",
    "    t_def = t_def_arrays[i-1]\n",
    "    p = xr.DataArray(p_def)\n",
    "    t = xr.DataArray(t_def)\n",
    "    q = xr.DataArray(q_def)\n",
    "    a = geo.relhum(t, q, p, use_gpu = False).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d892e2-66a7-42e6-8054-909d510ce5dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Xarray input and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a507eef-4dbf-4ef3-8099-e78b0194df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    p_def = p_def_arrays[i-1]\n",
    "    q_def = q_def_arrays[i-1]\n",
    "    t_def = t_def_arrays[i-1]\n",
    "    p = xr.DataArray(p_def)\n",
    "    t = xr.DataArray(t_def)\n",
    "    q = xr.DataArray(q_def)\n",
    "    a = geo.relhum(t, q, p, use_gpu = True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524bdcc-e83c-4482-979b-1abf1277ac74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Dask input with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1e244-71a8-4812-a8f5-0d3c3aaebb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8,9):\n",
    "    p_def = p_def_arrays[0]\n",
    "    q_def = q_def_arrays[0]\n",
    "    t_def = t_def_arrays[0]\n",
    "    p = xr.DataArray(p_def).chunk(10)\n",
    "    t = xr.DataArray(t_def).chunk(10)\n",
    "    q = xr.DataArray(q_def).chunk(10)\n",
    "    a = geo.relhum(t, q, p, use_gpu = False).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08135d44-53ff-4c5a-b1f5-6d437510d2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing Dask input with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0220d-520d-45c8-8db3-fc9a44ea136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    p_def = p_def_arrays[i-1]\n",
    "    q_def = q_def_arrays[i-1]\n",
    "    t_def = t_def_arrays[i-1]\n",
    "    p = xr.DataArray(p_def).chunk(10)\n",
    "    t = xr.DataArray(t_def).chunk(10)\n",
    "    q = xr.DataArray(q_def).chunk(10)\n",
    "    a = geo.relhum(t, q, p, use_gpu = True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc4745-d762-4eb4-b5ef-4dcb31ad0671",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unittests with CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed32a55-c85c-4fa7-bd45-3ff4395a0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test_relhum()\n",
    "\n",
    "test.setUpClass()\n",
    "test.test_float_input(use_gpu = False)\n",
    "test.test_list_input(use_gpu = False)\n",
    "test.test_numpy_input(use_gpu = False)\n",
    "test.test_dims_error()\n",
    "test.test_xarray_type_error()\n",
    "test.test_dask_compute(use_gpu = False)\n",
    "test.test_dask_lazy(use_gpu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e46b4-5d32-4ee4-b24c-dd4d90d57b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unittests with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c907e70-e181-4328-8083-7ac9ab53e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test_relhum()\n",
    "\n",
    "test.setUpClass()\n",
    "test.test_float_input(use_gpu = True)\n",
    "test.test_list_input(use_gpu = True)\n",
    "test.test_numpy_input(use_gpu = True)\n",
    "test.test_dims_error()\n",
    "test.test_xarray_type_error()\n",
    "test.test_dask_compute(use_gpu = True)\n",
    "test.test_dask_lazy(use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89402be5-a57e-4037-a487-e3523d2ae4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark Results for different Array Sizes (NUMPY/CUPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864afb6b-31c7-4f01-90ad-806fc289245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,max_power):\n",
    "    ArraySize = 10**i\n",
    "    p_def = p_def_arrays[i-1].data\n",
    "    t_def = t_def_arrays[i-1].data\n",
    "    q_def = q_def_arrays[i-1].data\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    #for numpy and cupy both\n",
    "    for xp in [np,cp]:\n",
    "        #calculation will be repeated 10 time to get the less biased performance results\n",
    "        repsize = 10\n",
    "        repeat = np.zeros([repsize])\n",
    "        for rep in range(0,repsize):\n",
    "            #create different sizes of arrays\n",
    "            if(xp == cp):\n",
    "                res_cupy = geo.relhum(t_def, q_def, p_def,True)\n",
    "                cp.cuda.runtime.deviceSynchronize()\n",
    "                time1 = time.time()\n",
    "                res_cupy = geo.relhum(t_def, q_def, p_def,True)\n",
    "                cp.cuda.runtime.deviceSynchronize()\n",
    "                time2 = time.time()\n",
    "                repeat[rep] = time2-time1\n",
    "            else:\n",
    "                time1 = time.time()\n",
    "                res_numpy = geo.relhum(t_def, q_def,p_def,False)\n",
    "                time2 = time.time()\n",
    "                repeat[rep] = time2-time1\n",
    "        #save times\n",
    "        data = {'Routine': np.repeat(Routine, repsize),\n",
    "                'Input':\"NumPy input\",\n",
    "                'Approach': np.repeat(xp.__name__ , repsize),\n",
    "                'ArraySize': np.repeat(ArraySize , repsize),\n",
    "                'iteration' : np.arange(1,repsize+1),\n",
    "                'Runtime(s)': repeat}\n",
    "\n",
    "        allData = pd.concat([allData,pd.DataFrame(data)], ignore_index=True)\n",
    "        print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "    test_validation(res_numpy,res_cupy)\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)\n",
    "plot(allData,\"Test_relhum_ported_numpy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2588d0b-fb99-4b53-a288-b8a4b6c86b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(allData,\"Test_relhum_ported_numpy.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcd16c-5c6c-4cad-9e38-73b9f26cbcd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results for different ArraySizes Xarray (with NumPy/CuPy arrays inside the Xarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a39d2-8527-4145-8ac9-f03b466c29ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Relhum on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8658fe1-d6ff-4187-90e4-4980769f41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCPU)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e2c6-2b68-488e-adef-6e0e9450d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvpath = \"relhum_ported_test_xarray.csv\"\n",
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,max_power):\n",
    "    ArraySize = 10**i\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    p_def = p_def_arrays[i-1]\n",
    "    t_def = t_def_arrays[i-1]\n",
    "    q_def = q_def_arrays[i-1]\n",
    "    #for numpy and cupy both\n",
    "    xp = np \n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "    #create different sizes of arrays\n",
    "        numpy_res = geo.relhum(t_def, q_def, p_def,False).compute()\n",
    "        time1 = time.time()\n",
    "        numpy_res = geo.relhum(t_def, q_def, p_def,False).compute()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "    numpy_results.append(numpy_res)\n",
    "    #save times\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with NumPy input\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    allData = pd.concat([allData,pd.DataFrame(data)], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a9679-29e3-4c11-8ae1-420f9e049814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Relhum on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39449d-eb3e-4b33-9092-9394c54ce360",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCUDA)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f990f-055b-4513-867d-1e094efe7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,max_power):\n",
    "    ArraySize = 10**i\n",
    "    p_def = p_def_arrays[i-1]\n",
    "    t_def = t_def_arrays[i-1]\n",
    "    q_def = q_def_arrays[i-1]\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    #for numpy and cupy both\n",
    "    xp = cp\n",
    "        #calculation will be repeated 10 time to get the less biased performance results\n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "        #create different sizes of arrays\n",
    "        cupy_res = geo.relhum(t_def, q_def, p_def,True).compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time1 = time.time()\n",
    "        cupy_res = geo.relhum(t_def, q_def, p_def,True).compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "    cupy_results.append(cupy_res)\n",
    "    #save times\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with NumPy input\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    allData = pd.concat([allData,pd.DataFrame(data)], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)\n",
    "# plot(allData,\"Relhum_xarray_np_ported.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a17406-2783-4389-b78e-8142cf27e5ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a3094-2533-40d0-b6d4-2a2783c88aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation \n",
    "for i in range(len(numpy_results)):\n",
    "    test_validation(cupy_results[i].data,numpy_results[i].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58db857-2ab8-4466-8340-67f03b646a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac048e-c86e-429b-880a-2526c5492581",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bc7fa-4fa1-4616-abff-5fe2ebe22114",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark Results for different ArraySizes Xarray (with Dask arrays inside the Xarray, then dask array type is either NumPy or CuPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fc679-e63d-4c66-bf0c-001720e0022f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Relhum on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e22a53-9c20-4995-9bc2-d319c17456a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCPU)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71296bd-7005-4099-9704-2aff49831259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvpath = \"relhum_ported_test_dask.csv\"\n",
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,max_power):\n",
    "    ArraySize = 10**i\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    p_def = p_def_arrays[i-1].chunk(chunksize)\n",
    "    t_def = t_def_arrays[i-1].chunk(chunksize)\n",
    "    q_def = q_def_arrays[i-1].chunk(chunksize)\n",
    "    #for numpy and cupy both\n",
    "    xp = np\n",
    "    #calculation will be repeated 10 time to get the less biased performance results\n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "        #create different sizes of arrays\n",
    "        numpy_res = geo.relhum(t_def, q_def, p_def,False).compute()\n",
    "        time1 = time.time()\n",
    "        numpy_res = geo.relhum(t_def, q_def, p_def,False).compute()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "        #save times\n",
    "    numpy_results.append(numpy_res)\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with Dask array input\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    allData = pd.concat([allData,pd.DataFrame(data)], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364591d-d6df-44e0-82e3-6aaac0481a7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Relhum on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c47630-a644-43cf-918c-a59caef436ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCUDA)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e801b0-367c-4000-88be-29db5abaecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,max_power):\n",
    "    ArraySize = 10**i\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    p_def = p_def_arrays[i-1].chunk(chunksize)\n",
    "    t_def = t_def_arrays[i-1].chunk(chunksize)\n",
    "    q_def = q_def_arrays[i-1].chunk(chunksize)\n",
    "    #for numpy and cupy both\n",
    "    xp = cp\n",
    "    #calculation will be repeated 10 time to get the less biased performance results\n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "        #create different sizes of arrays\n",
    "        cupy_res = geo.relhum(t_def, q_def, p_def,True).compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time1 = time.time()\n",
    "        cupy_res = geo.relhum(t_def, q_def, p_def,True).compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "        #save times\n",
    "    cupy_results.append(cupy_res)\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with Dask array input\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    new = pd.DataFrame(data)\n",
    "    allData = pd.concat([allData,new], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "    #print(np.allclose(cupy_res.data,numpy_res.data,atol=0.0000001))\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e202cd-1651-4c15-8c86-35e6c5dc83d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9bf6f-dea4-42d8-b125-7a94eb7942b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validation(res_numpy,res_cupy):\n",
    "    assert np.allclose(res_numpy,res_cupy, atol=0.0000001)\n",
    "#validation \n",
    "for i in range(len(numpy_results)):\n",
    "    test_validation(cupy_results[i].data,numpy_results[i].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b0f94-92cd-4e13-ae04-cbb5c75dac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_results[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42399d9-338b-4f91-a755-620504f97e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_results[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b2754-2bc1-44c3-8248-f3d434593155",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cupy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31393ae-590f-4179-9d6b-b41158730396",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Only comparing \"compute()\" runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f0d05-5a4e-459e-a9a5-c830d7d206f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0b661-f8e4-4d5f-924b-f3b84f064d6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7bfe2-8759-4d15-8a2b-c1016410ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCPU)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b546e4-0d12-424f-acd6-17c454ab1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvpath = \"relhum_ported_test_dask_compute_10.csv\"\n",
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,7):\n",
    "    ArraySize = 10**i\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    p_def = p_def_arrays[i-1].chunk(chunksize)\n",
    "    t_def = t_def_arrays[i-1].chunk(chunksize)\n",
    "    q_def = q_def_arrays[i-1].chunk(chunksize)\n",
    "    #for numpy and cupy both\n",
    "    xp = np\n",
    "    #calculation will be repeated 10 time to get the less biased performance results\n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "        #create different sizes of arrays\n",
    "        numpy_res = geo.relhum(t_def, q_def, p_def,False)\n",
    "        numpy_res.compute()\n",
    "        time1 = time.time()\n",
    "        numpy_res = numpy_res.compute()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "        #save times\n",
    "    numpy_results.append(numpy_res)\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with Dask array input compute\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    allData = pd.concat([allData,pd.DataFrame(data)], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a8849-0840-4664-93f1-e6385b05a282",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0724428-a3c2-45b3-a2b9-89faaa8e538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(clusterCUDA)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c885d0-a02d-48e5-b545-ae7b7f052349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_relhum main body\n",
    "Routine = \"Relhum\"\n",
    "print(Routine)\n",
    "allData = pd.DataFrame()\n",
    "#For different Array sizes\n",
    "for i in range(1,7):\n",
    "    ArraySize = 10**i\n",
    "    print(\"Array size: \", ArraySize)\n",
    "    p_def = p_def_arrays[i-1].chunk(chunksize)\n",
    "    t_def = t_def_arrays[i-1].chunk(chunksize)\n",
    "    q_def = q_def_arrays[i-1].chunk(chunksize)\n",
    "    #for numpy and cupy both\n",
    "    xp = cp\n",
    "    #calculation will be repeated 10 time to get the less biased performance results\n",
    "    repsize = 10\n",
    "    repeat = np.zeros([repsize])\n",
    "    for rep in range(0,repsize):\n",
    "        #create different sizes of arrays\n",
    "        cupy_res = geo.relhum(t_def, q_def, p_def,True)\n",
    "        cupy_res.compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time1 = time.time()\n",
    "        cupy_res = cupy_res.compute()\n",
    "        cp.cuda.runtime.deviceSynchronize()\n",
    "        time2 = time.time()\n",
    "        repeat[rep] = time2-time1\n",
    "        #save times\n",
    "    cupy_results.append(cupy_res)\n",
    "    data = {'Routine': np.repeat(Routine, repsize),\n",
    "            'Input':\"Xarray with Dask array input compute\",\n",
    "            'Approach': np.repeat(xp.__name__ , repsize),\n",
    "            'ArraySize': np.repeat(ArraySize , repsize),\n",
    "            'iteration' : np.arange(1,repsize+1),\n",
    "            'Runtime(s)': repeat}\n",
    "    new = pd.DataFrame(data)\n",
    "    allData = pd.concat([allData,new], ignore_index=True)\n",
    "    print(xp.__name__,np.mean(repeat), \"seconds\")\n",
    "    #print(np.allclose(cupy_res.data,numpy_res.data,atol=0.0000001))\n",
    "try:\n",
    "    previous = pd.read_csv(csvpath)\n",
    "    previous = pd.concat([previous,allData])\n",
    "except FileNotFoundError:\n",
    "    previous = allData\n",
    "previous.to_csv(csvpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970184b-f441-4b3e-8d4a-acd8d0f9a09e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fa41a-d9a8-4671-a851-4371003707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation \n",
    "for i in range(len(numpy_results)):\n",
    "    test_validation(cupy_results[i].data,numpy_results[i].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geocat]",
   "language": "python",
   "name": "conda-env-geocat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
